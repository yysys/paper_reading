# An Overview of Multi-Task Learning in Deep Neural Networks
机器学习方法常常用于优化一个特定的Score函数，或者商业性质的KPI目标。在实际中，实现上述目标常常采用单一模型或者集成模型。然而在实际中我们优化预测的目标常常不值一个，在推荐领域，除了关心CTR指标，点赞、分享等行为也值得预测优化。这种利用优化多个目标的任务被称为多任务学习MTL。

## 深度神经网络中的MTL

在神经网络中，有两种经典的MTL神经网络模型，分别是参数硬共享和参数软共享。
参数硬共享指的是不同任务底层的特征网络共享参数，参数硬共享如下图所示：
![png1](https://github.com/yysys/paper_reading/blob/master/images/1.png)
由于下层网络只有一套参数，参数硬共享容易造成优化目标的过拟合。过拟合的风险取决于任务数量N，直觉上，同时学习的任务越多，模型不得不去捕捉能够表示所有任务的特征表示，在原始任务上过拟合的风险就越小。
参数软共享是指每种任务都有一套自己的参数，加入正则化来约束底层网络的参数距离。参数软共享如下图所示：
![png2](https://github.com/yysys/paper_reading/blob/master/images/2.png)

## 为什么MTL有效果

MTL在一些任务上取得了不错的效果，下面是一些解释效果良好的原因：<br>
1.	MTL相当于扩展数据集，单独训练A任务可能有过拟合风险，然而联合训练A，B能获得更好的特征表达。<br>
2.	注意力：对于有许多噪声的数据，在A任务上表达良好的特征，可能在B任务上表达也良好。MTL相当于同时关注了不同任务的特征表达。<br>
3.	有些特征在A任务上容易学习，在B任务上却很难学习，MTL能够较为容易获得不同特征表示。<br>
4.	MTL能够帮助模型偏向各种任务都喜欢的特征表示，这有助于在更多任务的情况下也能较好学习模型。<br>
5.	MTL行为与正则化有些相似，有助于降低模型过拟合风险。<br>

## 最近的深度MTL神经网络
